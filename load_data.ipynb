{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a85b5b",
   "metadata": {},
   "source": [
    "### Write well, Im going to be using this for a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d93dd",
   "metadata": {},
   "source": [
    "#### Data we need:\n",
    "user input:\n",
    "- investment amount, trading pair -> amt0, amt1\n",
    "- start time and end time\n",
    "- time period that you assume fixed swap price, swap volumes or liquidity positions\n",
    "- upper and lower price\n",
    "- pool_fee_rate\n",
    "\n",
    "data from api:\n",
    "- cprice of each time period (tick, 1.0001 ** i)\n",
    "- L_pool at each time period at specific pool_fee_rate (liquidity?, or simply total X tokens + Y tokens in USD)\n",
    "- Swap volume at each time period at specific pool_fee_rate (volumeUSD?)\n",
    "- Gas cost to mint at each time period\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148c7cb",
   "metadata": {},
   "source": [
    "#### Fees\n",
    "The liquidity amount is calculated from the following numbers that describe a position: \n",
    "- amount of token 0 (amt0), amount of token 1 (amt1), \n",
    "- price (as x token 1's per token 0) at the upper limit of the position (upper), \n",
    "- price at the lower limit of the position (lower) \n",
    "- and the current swap price (cprice). \n",
    "\n",
    "Then liquidity (L_you?) for a position is calculated as follows:\n",
    "\n",
    "Case 1: cprice <= lower\n",
    "- liquidity = amt0 * (sqrt(upper) * sqrt(lower)) / (sqrt(upper) - sqrt(lower))\n",
    "\n",
    "Case 2: lower < cprice <= upper\n",
    "- liquidity is the min of the following two calculations:\n",
    "- amt0 * (sqrt(upper) * sqrt(cprice)) / (sqrt(upper) - sqrt(cprice))\n",
    "- amt1 / (sqrt(cprice) - sqrt(lower))\n",
    "\n",
    "Case 3: upper < cprice\n",
    "- liquidity = amt1 / (sqrt(upper) - sqrt(lower))\n",
    "\n",
    "Resources\n",
    "- liquidity can use this code: https://github.com/JNP777/UNI_V3-Liquitidy-amounts-calcs/blob/main/UNI_v3_funcs.py\n",
    "\n",
    "Fee is calculated by:\n",
    "- Fee income = (L_you/L_pool) * swap volume under fixed time period * pool_fee_rate/100\n",
    "- L_you also should be for that specific ticks only, not the whole amount you provided for. Its not linear, its calculated from the 3 cases above\n",
    "- Does Case1 and Case3's fee be 0 regardless?\n",
    "\n",
    "\n",
    "reference: https://uniswapv3.flipsidecrypto.com/\n",
    "- check my numbers with the reference from the website\n",
    "\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffdf4b",
   "metadata": {},
   "source": [
    "#### Impermanent Loss (is this v2 or v3)\n",
    "- IL (in %) = (2 sqrt(p) / (p+1) ) - 1\n",
    "- where p = r_t1/r_t2\n",
    "- and r_t is a price in b at time 1\n",
    "- Net $ loss = total asset value in dollars at stake time * IL (in%)\n",
    "\n",
    "reference: https://chainbulletin.com/impermanent-loss-explained-with-examples-math/#:~:text=Impermanent%20loss%20is%20the%20difference,is%20equal%20to%20200%20DAI\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd1b0d",
   "metadata": {},
   "source": [
    "#### Other cost\n",
    "\n",
    "Gas_costs_mint = 500000 gwei * gas_price at that time (??? double check actual cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81573320",
   "metadata": {},
   "source": [
    "### PNL/APR\n",
    "-PNL = Acumulated Fees_accrued (dolar value at generation) - IL - Gas_costs_mint\n",
    "\n",
    "-APR = PNL/Initial_capital*(age of the position / year time)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a40fd",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54835fd",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3524e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to use requests.post to make an API call to the subgraph url\n",
    "def run_query(q):\n",
    "\n",
    "    # endpoint where you are making the request\n",
    "    request = requests.post('https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'\n",
    "                            '',json={'query': q})\n",
    "    if request.status_code == 200:\n",
    "        return request\n",
    "    else:\n",
    "        raise Exception('Query failed. return code is {}.      {}'.format(request.status_code, query))\n",
    "        \n",
    "        \n",
    "# turns requests into dataframe        \n",
    "# def results_to_df(query_result):\n",
    "#     json_data_ = json.loads(query_result.text)\n",
    "#     df_data_ = json_data_['data']['pools']\n",
    "#     df_ = pd.DataFrame(df_data_)\n",
    "\n",
    "#     return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d351b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_id(symbol):\n",
    "    \n",
    "    # default should be first:10, in case there are more than 1 coins with the same symbol\n",
    "    query_ = \"\"\" \n",
    "    {{\n",
    "      tokens(first:1, where:{{symbol: \"{}\"}}) {{\n",
    "        id\n",
    "        symbol\n",
    "        name\n",
    "      }}\n",
    "    }}\"\"\".format(symbol)\n",
    "    \n",
    "    # run query\n",
    "    query_result_ = run_query(query_)\n",
    "    json_data_ = json.loads(query_result_.text)\n",
    "    \n",
    "    print(' ')\n",
    "    print('get_token_id: {}'.format(symbol))\n",
    "    print(json_data_)\n",
    "    \n",
    "    # make sure only return 1 object\n",
    "    if len(json_data_['data']['tokens']) == 1:\n",
    "        token_id_ = json_data_['data']['tokens'][0]['id']\n",
    "        return token_id_\n",
    "        \n",
    "    else:\n",
    "        print(json_data_['data'])\n",
    "        raise Exception('Returned number of token_ids != 1')\n",
    "\n",
    "        \n",
    "def get_pool_id(token0_id, token1_id, feeTier):\n",
    "    query_ = \"\"\"\n",
    "    {{\n",
    "      pools(first: 10, \n",
    "        where:{{token0: \"{}\",\n",
    "        token1: \"{}\",\n",
    "        feeTier:\"{}\" }}) \n",
    "      {{\n",
    "        id\n",
    "        token0{{symbol}}\n",
    "        token1{{symbol}}\n",
    "        feeTier\n",
    "      }}\n",
    "    }}\"\"\".format(token0_id, token1_id, feeTier)\n",
    "    \n",
    "    \n",
    "    # run query\n",
    "    query_result_ = run_query(query_)\n",
    "    json_data_ = json.loads(query_result_.text)\n",
    "    \n",
    "    print('\\n get_pool_id for feeTier: {}'.format(feeTier))\n",
    "    print(json_data_)\n",
    "    \n",
    "    # make sure there is only 1 pool that matches exactly\n",
    "    if len(json_data_['data']['pools']) == 1:\n",
    "        pool_id_ = json_data_['data']['pools'][0]['id']\n",
    "        return pool_id_\n",
    "    else:\n",
    "        print(json_data_['data'])\n",
    "        raise Exception('Returned number of token_ids != 1')\n",
    "\n",
    "        \n",
    "    return json_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b04f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poolDayDatas(pool_id, num_datapoints=1000):\n",
    "    # input: pool_id\n",
    "    # num_datapoints (must be multiple of max_request_)\n",
    "    \n",
    "    max_request_ = 1000\n",
    "    quotient_ = math.floor(num_datapoints/max_request_)\n",
    "#     remainder_ = num_datapoints%max_request_\n",
    "            \n",
    "    query_base_ = '''\n",
    "    {{\n",
    "      poolDayDatas(first:{},\n",
    "      skip: {},\n",
    "        where:{{ pool: \"{}\" }},\n",
    "      orderBy:date,\n",
    "      orderDirection: desc) \n",
    "      {{\n",
    "        date\n",
    "        tick\n",
    "        liquidity\n",
    "        volumeUSD\n",
    "        pool{{\n",
    "            token0{{\n",
    "                symbol\n",
    "            }}\n",
    "            token1{{\n",
    "                symbol\n",
    "            }}\n",
    "        }}\n",
    "      }}\n",
    "    }}'''\n",
    "    \n",
    "    poolDayDatas_array_ = []\n",
    "    \n",
    "    # query loop\n",
    "    for i in range(quotient_):\n",
    "        q_first_ = max_request_\n",
    "        q_next_ = i*max_request_\n",
    "        query_ = query_base_.format(q_first_, q_next_, pool_id)\n",
    "        query_result_ = run_query(query_)\n",
    "        json_data_ = json.loads(query_result_.text)\n",
    "#         print(json_data_)\n",
    "        poolDayDatas_array_ += json_data_['data']['poolDayDatas']\n",
    "    \n",
    "    print(' ')\n",
    "    print('\\n Queried PoolDayDatas, total of {} datapoints'.format(str(len(poolDayDatas_array_))))\n",
    "#     print('example:')\n",
    "#     print(poolDayDatas_array_[0])\n",
    "    \n",
    "    # array to dataframe\n",
    "    df_ = pd.json_normalize(poolDayDatas_array_)\n",
    "    df_.drop_duplicates(subset=['date']) \n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39196a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poolHourDatas(pool_id, num_datapoints=3000):\n",
    "    # input: pool_id\n",
    "    # num_datapoints (must be multiple of max_request_)\n",
    "    \n",
    "    max_request_ = 1000\n",
    "    quotient_ = math.floor(num_datapoints/max_request_)\n",
    "#     remainder_ = num_datapoints%max_request_\n",
    "            \n",
    "    query_base_ = '''\n",
    "    {{\n",
    "      poolHourDatas(first:{},\n",
    "      skip: {},\n",
    "        where:{{ pool: \"{}\" }},\n",
    "      orderBy:periodStartUnix,\n",
    "      orderDirection: desc) \n",
    "      {{\n",
    "        periodStartUnix\n",
    "        pool{{\n",
    "            token0{{\n",
    "                symbol\n",
    "            }}\n",
    "            token1{{\n",
    "                symbol\n",
    "            }}\n",
    "        }}\n",
    "        liquidity\n",
    "        sqrtPrice\n",
    "        token0Price\n",
    "        token1Price\n",
    "        tick\n",
    "        feeGrowthGlobal0X128\n",
    "        feeGrowthGlobal1X128\n",
    "        tvlUSD\n",
    "        volumeToken0\n",
    "        volumeToken1\n",
    "        volumeUSD\n",
    "        feesUSD\n",
    "        txCount\n",
    "        open\n",
    "        high\n",
    "        low\n",
    "        close\n",
    "      }}\n",
    "    }}'''\n",
    "    \n",
    "    poolDayDatas_array_ = []\n",
    "    \n",
    "    # query loop\n",
    "    for i in range(quotient_):\n",
    "        q_first_ = max_request_\n",
    "        q_next_ = i*max_request_\n",
    "        query_ = query_base_.format(q_first_, q_next_, pool_id)\n",
    "        query_result_ = run_query(query_)\n",
    "        json_data_ = json.loads(query_result_.text)\n",
    "#         print(json_data_)\n",
    "        try:\n",
    "            poolDayDatas_array_ += json_data_['data']['poolHourDatas']\n",
    "        except Exception:\n",
    "#             print('.. Pass')\n",
    "            pass\n",
    "    \n",
    "    print(' ')\n",
    "    print('\\n Queried poolHourDatas, total of {} datapoints'.format(str(len(poolDayDatas_array_))))\n",
    "#     print('example:')\n",
    "#     print(poolDayDatas_array_[0])\n",
    "    \n",
    "    # array to dataframe\n",
    "    df_ = pd.json_normalize(poolDayDatas_array_)\n",
    "#     df_.drop_duplicates(subset=['periodStartUnix']) # TODO: BUGGGG ??\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f48c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swaps(pool_id, time_start='1627369200', time_end='1623772800', num_datapoints=6000):\n",
    "    # input: pool_id\n",
    "    # num_datapoints (must be multiple of max_request_)\n",
    "    \n",
    "    max_request_ = 1000\n",
    "    quotient_ = math.floor(num_datapoints/max_request_)\n",
    "#     remainder_ = num_datapoints%max_request_\n",
    "           \n",
    "    query_base_ = '''\n",
    "    {{\n",
    "      swaps(first:{}, skip: {},\n",
    "            where:{{ pool: \"{}\",\n",
    "            timestamp_lt: \"{}\",\n",
    "            timestamp_gt: \"{}\"}},\n",
    "          orderBy:timestamp,\n",
    "          orderDirection: desc){{\n",
    "        transaction {{\n",
    "          blockNumber\n",
    "          timestamp\n",
    "          gasUsed\n",
    "          gasPrice\n",
    "        }}\n",
    "        id\n",
    "        timestamp\n",
    "        tick\n",
    "        amount0\n",
    "        amount1\n",
    "        amountUSD\n",
    "        sqrtPriceX96\n",
    "      }}\n",
    "    }}'''\n",
    "    \n",
    "    swap_arrays_ = []\n",
    "    \n",
    "    # query loop\n",
    "    for i in range(quotient_):\n",
    "        q_first_ = max_request_\n",
    "        q_next_ = i*max_request_\n",
    "        query_ = query_base_.format(q_first_, q_next_, pool_id, time_start, time_end)\n",
    "        query_result_ = run_query(query_)\n",
    "        json_data_ = json.loads(query_result_.text)\n",
    "        \n",
    "        try:\n",
    "            swap_arrays_ += json_data_['data']['swaps']\n",
    "            \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    print('Queried Swaps, total of {} datapoints'.format(str(len(swap_arrays_))))\n",
    "    print(' ')\n",
    "    df_ = pd.json_normalize(swap_arrays_)\n",
    "    \n",
    "    # next time start, if at the edge, then we keep looping and skipping\n",
    "    if len(swap_arrays_) != 0:\n",
    "        # last element of timestamp, add 1 so next iterations still includes it\n",
    "        next_time_start_ = str( int(df_['timestamp'][df_.index[-1]]) + 1 )\n",
    "    else:\n",
    "        next_time_start_ = time_start\n",
    "            \n",
    "            \n",
    "    return df_, next_time_start_\n",
    "\n",
    "\n",
    "# get_swap can only request 6000 datapoints at the time. this is to loop get_swap to get more data\n",
    "def get_swaps_loop(pool_id, time_start='1627369200', time_end='1623772800'): # ,  num_datapoints= 150000\n",
    "    \n",
    "    print('time_start: {}, time_end: {}'.format(time_start, time_end))\n",
    "    \n",
    "    max_num_query = 6000\n",
    "#     num_iterations = math.floor(num_datapoints/max_num_query) + 3 # add 3 just in case\n",
    "    \n",
    "    next_time_start_ = time_start\n",
    "    count = 0 # counting number of times that data returns is less than maximum, meaning reaching the end\n",
    "#     for i in range(num_iterations):\n",
    "    first_time = True\n",
    "    while(count<10):\n",
    "        print('next_time_start_: {}'.format(next_time_start_))\n",
    "        df_, next_time_start_ = get_swaps(pool_id, next_time_start_, time_end, num_datapoints=max_num_query)\n",
    "        \n",
    "        if first_time == True:\n",
    "            df_all_ = df_\n",
    "            first_time = False\n",
    "        else:\n",
    "            df_all_ = df_all_.append(df_)\n",
    "            \n",
    "        if df_.shape[0] < 6000:\n",
    "            count += 1\n",
    "    \n",
    "    # drop duplicates, reset index\n",
    "    df_all_ = df_all_.drop_duplicates(subset=['id'])\n",
    "    df_all_ = df_all_.reset_index(drop=True)\n",
    "    print('Total swaps data = {}'.format(df_all_.shape[0]))\n",
    "    \n",
    "    return df_all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0372fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_poolHourData_swaps_all(df_poolHourDatas, df_swaps_all):\n",
    "    \n",
    "    # Match timestamp with hour period, and assign to df_swaps_all['periodStartUnix']\n",
    "    def compute_periodStartUnix(row_):\n",
    "        return row_['timestamp'] - (row_['timestamp'] % 3600)\n",
    "    def compute_periodEndUnix(row_):\n",
    "        return row_['periodStartUnix'] + 3600\n",
    "\n",
    "    df_swaps_all['timestamp'] = df_swaps_all['timestamp'].astype(int)    \n",
    "    df_swaps_all['periodStartUnix'] = df_swaps_all.apply(lambda row: compute_periodStartUnix(row), axis=1)                       \n",
    "    df_swaps_all['periodEndUnix'] = df_swaps_all.apply(lambda row: compute_periodEndUnix(row), axis=1)                       \n",
    "\n",
    "    df_swaps_all['periodStartUnix'] = df_swaps_all['periodStartUnix'].astype(int)        \n",
    "    df_swaps_all['periodEndUnix'] = df_swaps_all['periodEndUnix'].astype(int)\n",
    "    \n",
    "    # Create swaps_txCount to compare with txCount in poolHourDatas to check integrity\n",
    "    df_swaps_all['swaps_txCount'] = 1\n",
    "    \n",
    "    # TODO -------------------------------------------------------------\n",
    "    df_swaps_all['amount0_p'] = df_swaps_all['amount0'].apply(lambda x: x if x > 0 else 0)\n",
    "    df_swaps_all['amount0_n'] = df_swaps_all['amount0'].apply(lambda x: x if x <= 0 else 0)\n",
    "    df_swaps_all['amount1_p'] = df_swaps_all['amount1'].apply(lambda x: x if x > 0 else 0)\n",
    "    df_swaps_all['amount1_n'] = df_swaps_all['amount1'].apply(lambda x: x if x <= 0 else 0)\n",
    "    \n",
    "    \n",
    "    # Groupby->Sum based on periodStartUnix, specify columns to sum at GROUPBY_COLS\n",
    "    GROUPBY_COLS = ['periodStartUnix','amount0', 'amount1', \n",
    "                    'amount0_p', 'amount0_n', 'amount1_p', 'amount1_n',\n",
    "                    'amountUSD', 'swaps_txCount']\n",
    "    df_swaps_to_merge = df_swaps_all[GROUPBY_COLS]\n",
    "    df_swaps_to_merge = df_swaps_to_merge.astype({'periodStartUnix': 'int',\n",
    "                                                 'amount0':'float','amount1':'float', \n",
    "                                                  'amount0_p':'float','amount0_n':'float',\n",
    "                                                  'amount1_p':'float', 'amount1_n':'float',\n",
    "                                                  'amountUSD':'float', 'swaps_txCount':'int'})\n",
    "    df_swaps_to_merge = df_swaps_to_merge.groupby(by=['periodStartUnix']).sum()\n",
    "\n",
    "    # Merge df_swaps_all (groupby) with df_poolHourDatas\n",
    "    df_poolHourDatas['periodStartUnix'] = df_poolHourDatas['periodStartUnix'].astype(int)\n",
    "    df_merged = df_poolHourDatas.merge(df_swaps_to_merge, how='left', on='periodStartUnix')\n",
    "    df_merged['txCount'] = df_merged['txCount'].astype(int)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5ebbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(token0, token1, feeTier):\n",
    "    \n",
    "    # Indicate Tokens and FeeTier\n",
    "    token0_id = get_token_id(token0)\n",
    "    token1_id = get_token_id(token1)\n",
    "    pool_id = get_pool_id(token0_id, token1_id, feeTier)\n",
    "\n",
    "    # Get poolHourDatas\n",
    "    df_poolHourDatas = get_poolHourDatas(pool_id, num_datapoints=10000)\n",
    "\n",
    "    # Get Swap Datas within the poolHourDatas timeframe\n",
    "    time_start = df_poolHourDatas['periodStartUnix'][0]\n",
    "    time_end = df_poolHourDatas['periodStartUnix'][df_poolHourDatas.index[-1]]\n",
    "    \n",
    "    # Get Swaps data\n",
    "    df_swaps_all = get_swaps_loop(pool_id, time_start, time_end) # ,  num_datapoints= 150000\n",
    "    \n",
    "    # Saving settings\n",
    "    SETTINGS = '{}-{}-{}-timestamp-{}-{}.csv'.format(token0, token1, feeTier, time_start, time_end)\n",
    "    df_swaps_all.to_csv('../data/df_swaps_all_'+SETTINGS)\n",
    "    df_poolHourDatas.to_csv('../data/df_poolHourDatas_'+SETTINGS)\n",
    "    \n",
    "    # Merge data\n",
    "    df_merged = merge_poolHourData_swaps_all(df_poolHourDatas, df_swaps_all)\n",
    "    \n",
    "    df_merged.to_csv('../data/df_merged_tmp_'+SETTINGS)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dbc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75965049",
   "metadata": {},
   "source": [
    "### Test running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec4e0b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "get_token_id: DAI\n",
      "{'data': {'tokens': [{'id': '0x6b175474e89094c44da98b954eedeac495271d0f', 'name': 'Dai Stablecoin', 'symbol': 'DAI'}]}}\n",
      " \n",
      "get_token_id: WETH\n",
      "{'data': {'tokens': [{'id': '0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2', 'name': 'Wrapped Ether', 'symbol': 'WETH'}]}}\n",
      "\n",
      " get_pool_id for feeTier: 3000\n",
      "{'data': {'pools': [{'feeTier': '3000', 'id': '0xc2e9f25be6257c210d7adf0d4cd6e3e881ba25f8', 'token0': {'symbol': 'DAI'}, 'token1': {'symbol': 'WETH'}}]}}\n",
      " \n",
      "\n",
      " Queried poolHourDatas, total of 2184 datapoints\n",
      "time_start: 1628067600, time_end: 1620158400\n",
      "next_time_start_: 1628067600\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1625901723\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1624309658\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1623167107\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1622264767\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1621810241\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1621421454\n",
      "Queried Swaps, total of 6000 datapoints\n",
      " \n",
      "next_time_start_: 1620644083\n",
      "Queried Swaps, total of 1558 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "next_time_start_: 1620159676\n",
      "Queried Swaps, total of 1 datapoints\n",
      " \n",
      "Total swaps data = 43550\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kd/nddnd1mj1pqbmtxx_7nkdkjh0000gn/T/ipykernel_16349/1902125526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeeTier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOKEN0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKEN1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeTier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/kd/nddnd1mj1pqbmtxx_7nkdkjh0000gn/T/ipykernel_16349/2329046002.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(token0, token1, feeTier)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Merge data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_poolHourData_swaps_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_poolHourDatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/df_merged_tmp_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mSETTINGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kd/nddnd1mj1pqbmtxx_7nkdkjh0000gn/T/ipykernel_16349/1649694558.py\u001b[0m in \u001b[0;36mmerge_poolHourData_swaps_all\u001b[0;34m(df_poolHourDatas, df_swaps_all)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# TODO -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0_p'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0_n'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount1_p'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4354\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \"\"\"\n\u001b[0;32m-> 4356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4358\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1090\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1093\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/kd/nddnd1mj1pqbmtxx_7nkdkjh0000gn/T/ipykernel_16349/1649694558.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# TODO -------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0_p'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0_n'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount1_p'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swaps_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "TOKEN0 = 'DAI'\n",
    "TOKEN1 = 'WETH'\n",
    "feeTier = '3000' \n",
    "\n",
    "df_merged = get_data(TOKEN0, TOKEN1, feeTier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344ee7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poolHourDatas = pd.read_csv('../data/df_poolHourDatas_DAI-WETH-3000-timestamp-1628067600-1620158400.csv')\n",
    "\n",
    "\n",
    "df_swaps_all = pd.read_csv('../data/df_swaps_all_DAI-WETH-3000-timestamp-1628067600-1620158400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d914937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_poolHourData_swaps_all(df_poolHourDatas, df_swaps_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f154fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amountUSD</th>\n",
       "      <th>amount0</th>\n",
       "      <th>amount0_p</th>\n",
       "      <th>amount0_n</th>\n",
       "      <th>amount1</th>\n",
       "      <th>amount1_p</th>\n",
       "      <th>amount1_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57500.601976</td>\n",
       "      <td>-57459.203778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-57459.203778</td>\n",
       "      <td>2.328719e+01</td>\n",
       "      <td>2.328719e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86047.419323</td>\n",
       "      <td>-85905.862610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-85905.862610</td>\n",
       "      <td>3.467641e+01</td>\n",
       "      <td>3.467641e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>362900.901437</td>\n",
       "      <td>-311575.151808</td>\n",
       "      <td>25410.170014</td>\n",
       "      <td>-336985.321821</td>\n",
       "      <td>1.250536e+02</td>\n",
       "      <td>1.352044e+02</td>\n",
       "      <td>-10.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491576.254829</td>\n",
       "      <td>337791.453639</td>\n",
       "      <td>414742.684089</td>\n",
       "      <td>-76951.230449</td>\n",
       "      <td>-1.345576e+02</td>\n",
       "      <td>3.081296e+01</td>\n",
       "      <td>-165.370582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-331.043259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-331.043259</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-33.854156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-33.854156</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2184 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          amountUSD        amount0      amount0_p      amount0_n  \\\n",
       "0               NaN            NaN            NaN            NaN   \n",
       "1      57500.601976  -57459.203778       0.000000  -57459.203778   \n",
       "2      86047.419323  -85905.862610       0.000000  -85905.862610   \n",
       "3     362900.901437 -311575.151808   25410.170014 -336985.321821   \n",
       "4     491576.254829  337791.453639  414742.684089  -76951.230449   \n",
       "...             ...            ...            ...            ...   \n",
       "2179       0.000000    -331.043259       0.000000    -331.043259   \n",
       "2180            NaN            NaN            NaN            NaN   \n",
       "2181            NaN            NaN            NaN            NaN   \n",
       "2182       0.000000      -0.000336       0.000000      -0.000336   \n",
       "2183       0.000000     -33.854156       0.000000     -33.854156   \n",
       "\n",
       "           amount1     amount1_p   amount1_n  \n",
       "0              NaN           NaN         NaN  \n",
       "1     2.328719e+01  2.328719e+01    0.000000  \n",
       "2     3.467641e+01  3.467641e+01    0.000000  \n",
       "3     1.250536e+02  1.352044e+02  -10.150800  \n",
       "4    -1.345576e+02  3.081296e+01 -165.370582  \n",
       "...            ...           ...         ...  \n",
       "2179  1.000000e-01  1.000000e-01    0.000000  \n",
       "2180           NaN           NaN         NaN  \n",
       "2181           NaN           NaN         NaN  \n",
       "2182  1.000000e-07  1.000000e-07    0.000000  \n",
       "2183  1.000000e-02  1.000000e-02    0.000000  \n",
       "\n",
       "[2184 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[['amountUSD','amount0','amount0_p','amount0_n','amount1','amount1_p','amount1_n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eee0e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('../data/df_merged_tmp_DAI-WETH-3000-timestamp-1627574400-1620169200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559266cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21499e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c148f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_list = ['periodStartUnix',  \n",
    "              'txCount', 'swaps_txCount', # check data integrity\n",
    "              'amount0', 'amount1', 'amountUSD', # swaps data\n",
    "              'tick', 'liquidity', 'sqrtPrice', 'tvlUSD', # pool data at that hour\n",
    "              'pool.token0.symbol', 'pool.token1.symbol', # token data\n",
    "              'token0Price', 'token1Price'\n",
    "             ]\n",
    "df_merged[watch_list].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eye check:\n",
    "# print(df_poolHourDatas.shape)\n",
    "# print(df_swaps_all.shape)\n",
    "print(df_merged.shape)\n",
    "\n",
    "print(df_merged['txCount'].sum())\n",
    "print(df_merged['swaps_txCount'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbe579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b05673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207facd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d2a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74932c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62202a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187721f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee166d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552916d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0697f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1923dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f93ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99848ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d681bc7d",
   "metadata": {},
   "source": [
    "##### Query tokens with symbol\n",
    "{\n",
    "  tokens(first:10, where:{symbol: \"WETH\"}) {\n",
    "    id\n",
    "    symbol\n",
    "    name\n",
    "  }\n",
    "}\n",
    "\n",
    "##### Query pools with token0 id,  token1 ids and feeTiers\n",
    "{\n",
    "  pools(first:10, \n",
    "    where:{token0:\"0x6b175474e89094c44da98b954eedeac495271d0f\",\n",
    "    token1: \"0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2\",\n",
    "    feeTier:\"3000\" }) \n",
    "  {\n",
    "    id\n",
    "    token0{symbol}\n",
    "    token1{symbol}\n",
    "    feeTier\n",
    "  }\n",
    "}\n",
    "\n",
    "##### Query poolDayDatas with pool id, order by date - Needs to be iterative (max 1000 query)\n",
    "{\n",
    "  poolDayDatas(first:1000,\n",
    "  next: 1000,\n",
    "    where:{pool:\"0xa80964c5bbd1a0e95777094420555fead1a26c1e\"},\n",
    "  orderBy:date,\n",
    "  orderDirection: desc) \n",
    "  {\n",
    "    date\n",
    "    tick\n",
    "    liquidity\n",
    "    volumeUSD\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Query examples on filtering\n",
    "\n",
    "{\n",
    "  pools\n",
    "  (first: 10, \n",
    "    where: {liquidity_gt: \"1000000\", \n",
    "      feeTier: \"10000\"}\n",
    "    orderBy: liquidity, \n",
    "    orderDirection: desc)\n",
    "  {\n",
    "    token0{symbol}\n",
    "    token1{symbol}\n",
    "    liquidity\n",
    "  }\n",
    "\n",
    "\n",
    "(token0) DAI id = 0x6b175474e89094c44da98b954eedeac495271d0f\n",
    "(token1) WETH id = 0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2\n",
    "(feeTier) \"3000\"\n",
    "\n",
    "(DAI-WETH 500) Pool id = 0x60594a405d53811d3bc4766596efd80fd545a270\n",
    "(DAI-WETH 3000) Pool id = 0xc2e9f25be6257c210d7adf0d4cd6e3e881ba25f8\n",
    "(DAI-WETH 1000) Pool id = 0xa80964c5bbd1a0e95777094420555fead1a26c1e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b64c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce4fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530b6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393158c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18206f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
